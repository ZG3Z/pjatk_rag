{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, load_index_from_storage, Settings\n",
    "from llama_index.core.node_parser import TokenTextSplitter, SentenceSplitter, SemanticSplitterNodeParser, HierarchicalNodeParser, SentenceWindowNodeParser, get_leaf_nodes\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.indices.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.core.extractors import QuestionsAnsweredExtractor, TitleExtractor\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine, RetrieverQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import phoenix as px\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from phoenix.trace import using_project\n",
    "\n",
    "import functions\n",
    "\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]='true'\n",
    "\n",
    "data_path = '../content/data'\n",
    "storage_path = \"../content/storage\"\n",
    "testset_path = '../content/testset/testset.csv'\n",
    "\n",
    "ENDPOINT = 'http://127.0.0.1:6006/v1/traces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(data_path).load_data()\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.1)\n",
    "Settings.embed_model = OpenAIEmbedding()\n",
    "eval_model = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = functions.generate_testset(testset_path, data_path)\n",
    "testset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app()\n",
    "tracer_provider = TracerProvider()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(ENDPOINT)))\n",
    "\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "client = px.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with using_project(\"indexing-token_splitter\"):\n",
    "    token_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)\n",
    "    token_splitter_index = functions.build_index(storage_path, 'token_splitter', documents, [token_splitter])\n",
    "    token_splitter_engine = token_splitter_index.as_query_engine(similarity_top_k=5)\n",
    "    result_token_splitter, eval_token_splitter = functions.evaluation('querying-token_splitter', token_splitter_engine, testset, eval_model)\n",
    "    eval_token_splitter, eval_token_splitter_scores = functions.add_evaluation(client, 'querying-token_splitter', eval_token_splitter)\n",
    "\n",
    "with using_project(\"indexing-sentence_splitter\"):\n",
    "    sentence_splitter = SentenceSplitter(chunk_size=512)\n",
    "    sentence_splitter_index = functions.build_index(storage_path, 'sentence_splitter', documents, [sentence_splitter])\n",
    "    sentence_splitter_engine = sentence_splitter_index.as_query_engine(similarity_top_k=5)\n",
    "    result_sentence_splitter, eval_sentence_splitter = functions.evaluation('querying-sentence_splitter', sentence_splitter_engine, testset, eval_model)\n",
    "    eval_sentence_splitter, eval_sentence_splitter_scores = functions.add_evaluation(client, 'querying-sentence_splitter', eval_sentence_splitter)\n",
    "\n",
    "with using_project(\"indexing-semantic_splitter\"):\n",
    "    semantic_splitter = SemanticSplitterNodeParser(buffer_size=1, embed_model=Settings.embed_model)\n",
    "    semantic_splitter_index = functions.build_index(storage_path, 'semantic_splitter', documents, [semantic_splitter])\n",
    "    semantic_splitter_engine = semantic_splitter_index.as_query_engine(similarity_top_k=5)\n",
    "    result_semantic_splitter, eval_semantic_splitter = functions.evaluation('querying-semantic_splitter', semantic_splitter_engine, testset, eval_model)\n",
    "    eval_semantic_splitter, eval_semantic_splitter_scores = functions.add_evaluation(client, 'querying-semantic_splitter', eval_semantic_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['Token', 'Sentence', 'Semantic']\n",
    "scores = [eval_token_splitter, eval_sentence_splitter, eval_semantic_splitter]\n",
    "evals = functions.create_results(types, scores)\n",
    "functions.plot_aggregate_evaluation(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with using_project(\"indexing-hierarchical_node_parser\"):\n",
    "    storage_name = 'hierarchical_node_parser'\n",
    "    if not os.path.exists(storage_path+str(f'/{storage_name}')):\n",
    "        node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[2048, 512, 128])\n",
    "        nodes = node_parser.get_nodes_from_documents(documents)\n",
    "        leaf_nodes = get_leaf_nodes(nodes)\n",
    "        automerging_index = VectorStoreIndex(leaf_nodes)\n",
    "        automerging_index.storage_context.persist(persist_dir=storage_path+str(f'/{storage_name}')) \n",
    "    else:\n",
    "        automerging_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=storage_path+str(f'/{storage_name}')))\n",
    "\n",
    "    automerging_retriever = automerging_index.as_retriever(similarity_top_k=12)\n",
    "    retriever = AutoMergingRetriever(automerging_retriever, automerging_index.storage_context)\n",
    "    auto_merging_engine = RetrieverQueryEngine.from_args(automerging_retriever)\n",
    "    result_auto_merging, eval_auto_merging = functions.evaluation('querying-auto_merging', auto_merging_engine, testset, eval_model)\n",
    "    eval_auto_merging, eval_auto_merging_scores = functions.add_evaluation(client, 'querying-auto_merging', eval_auto_merging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with using_project(\"indexing-sentence_window\"):\n",
    "    sentence_window = SentenceWindowNodeParser.from_defaults(window_size=3, window_metadata_key=\"window\", original_text_metadata_key=\"original_text\")\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    sentence_window_index = functions.build_index(storage_path, 'sentence_window', documents, [sentence_window])\n",
    "    sentence_window_engine = sentence_window_index.as_query_engine(similarity_top_k=5, node_postprocessors=[postproc])\n",
    "    result_sentence_window, eval_sentence_window = functions.evaluation('querying-sentence_window', sentence_window_engine, testset, eval_model)\n",
    "    eval_sentence_window, eval_sentence_window_scores = functions.add_evaluation(client, 'querying-sentence_window', eval_sentence_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types.append('AutoMerging')\n",
    "scores.append(eval_auto_merging)\n",
    "\n",
    "types.append('Sentence window')\n",
    "scores.append(eval_sentence_window)\n",
    "\n",
    "evals = functions.create_results(types, scores)\n",
    "functions.plot_aggregate_evaluation(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [eval_token_splitter_scores, eval_sentence_splitter_scores, eval_semantic_splitter_scores, eval_auto_merging_scores, eval_sentence_window_scores]\n",
    "\n",
    "functions.plot_individual_evaluation(dfs, 'context_precision')\n",
    "functions.plot_individual_evaluation(dfs, 'faithfulness')\n",
    "functions.plot_individual_evaluation(dfs, 'answer_relevancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with using_project(\"extracting-metadata\"):\n",
    "    title_extractor = TitleExtractor(nodes=3)\n",
    "    qa_extractor = QuestionsAnsweredExtractor(questions=2)\n",
    "    metadata_index = functions.build_index(storage_path, 'metadata_token', documents, [sentence_splitter, title_extractor, qa_extractor])\n",
    "    metadata_engine = metadata_index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "    result_metadata, eval_metadata = functions.evaluation('querying-metadata', metadata_engine, testset, eval_model)\n",
    "    eval_metadata, eval_metadata_scores = functions.add_evaluation(client, 'querying-metadata', eval_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types.append('Metadata')\n",
    "scores.append(eval_metadata)\n",
    "\n",
    "evals = functions.create_results(types, scores)\n",
    "functions.plot_aggregate_evaluation(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_engine = TransformQueryEngine(token_splitter_engine, hyde)\n",
    "\n",
    "result_hyde, eval_hyde = functions.evaluation('querying-hyde', hyde_engine, testset, eval_model)\n",
    "eval_hyde, eval_hyde_scores = functions.add_evaluation(client, 'querying-hyde', eval_hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_hyde, eval_hyde = functions.evaluation('querying-hyde-simple', hyde_engine, testset[testset.evolution_type == 'simple'], eval_model)\n",
    "eval_hyde, eval_hyde_scores = functions.add_evaluation(client, 'querying-hyde-simple', eval_hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_hyde, eval_hyde = functions.evaluation('querying-hyde_reasoning', hyde_engine, testset[testset.evolution_type == 'reasoning'], eval_model)\n",
    "eval_hyde, eval_hyde_scores = functions.add_evaluation(client, 'querying-hyde_reasoning', eval_hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_hyde, eval_hyde = functions.evaluation('querying-hyde_multi_context', hyde_engine, testset[testset.evolution_type == 'multi_context'], eval_model)\n",
    "eval_hyde, eval_hyde_scores = functions.add_evaluation(client, 'querying-hyde_multi_context', eval_hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = QueryEngineTool.from_defaults(query_engine=token_splitter_engine)\n",
    "subquestion_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=[tool])\n",
    "\n",
    "result_subquestion, eval_subquestion = functions.evaluation('querying-subquestion', subquestion_engine, testset, eval_model)\n",
    "eval_subquestion, eval_subquestion_scores = functions.add_evaluation(client, 'querying-subquestion', eval_subquestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = QueryEngineTool.from_defaults(query_engine=token_splitter_engine)\n",
    "subquestion_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=[tool])\n",
    "\n",
    "result_subquestion, eval_subquestion = functions.evaluation('querying-subquestion-simple', subquestion_engine, testset[testset.evolution_type == 'simple'], eval_model)\n",
    "eval_subquestion, eval_subquestion_scores = functions.add_evaluation(client, 'querying-subquestion-simple', eval_subquestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = QueryEngineTool.from_defaults(query_engine=token_splitter_engine)\n",
    "subquestion_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=[tool])\n",
    "\n",
    "result_subquestion, eval_subquestion = functions.evaluation('querying-subquestion-reasoning', subquestion_engine, testset[testset.evolution_type == 'reasoning'], eval_model)\n",
    "eval_subquestion, eval_subquestion_scores = functions.add_evaluation(client, 'querying-subquestion-reasoning', eval_subquestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = QueryEngineTool.from_defaults(query_engine=token_splitter_engine)\n",
    "subquestion_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=[tool])\n",
    "\n",
    "result_subquestion, eval_subquestion = functions.evaluation('querying-subquestion-multi_context', subquestion_engine, testset[testset.evolution_type == 'multi_context'], eval_model)\n",
    "eval_subquestion, eval_subquestion_scores = functions.add_evaluation(client, 'querying-subquestion-multi_context', eval_subquestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types.append('HyDE')\n",
    "scores.append(eval_hyde)\n",
    "\n",
    "types.append('SubQuest')\n",
    "scores.append(eval_subquestion)\n",
    "\n",
    "evals = functions.create_results(types, scores)\n",
    "functions.plot_aggregate_evaluation(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_rereanker_base = SentenceTransformerRerank(model=\"BAAI/bge-reranker-base\", top_n=2)\n",
    "bge_reranker_large = SentenceTransformerRerank(model=\"BAAI/bge-reranker-large\", top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = functions.build_index(storage_path, 'token_splitter', documents, [sentence_splitter, title_extractor, qa_extractor])\n",
    "base_engine = base_index.as_query_engine(similarity_top_k=5, node_postprocessors=[bge_rereanker_base])\n",
    "result_base, eval_base = functions.evaluation('querying-base', base_engine, testset, eval_model)\n",
    "eval_base, eval_base_scores = functions.add_evaluation(client, 'querying-base', eval_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = functions.build_index(storage_path, 'token_splitter', documents, [sentence_splitter, title_extractor, qa_extractor])\n",
    "base_engine = base_index.as_query_engine(similarity_top_k=5, node_postprocessors=[bge_rereanker_base])\n",
    "result_base, eval_base = functions.evaluation('querying-base-simple', base_engine, testset[testset.evolution_type == 'simple'], eval_model)\n",
    "eval_base, eval_base_scores = functions.add_evaluation(client, 'querying-base-simple', eval_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = functions.build_index(storage_path, 'token_splitter', documents, [sentence_splitter, title_extractor, qa_extractor])\n",
    "base_engine = base_index.as_query_engine(similarity_top_k=5, node_postprocessors=[bge_rereanker_base])\n",
    "result_base, eval_base = functions.evaluation('querying-base-reasoning', base_engine, testset[testset.evolution_type == 'reasoning'], eval_model)\n",
    "eval_base, eval_base_scores = functions.add_evaluation(client, 'querying-base-reasoning', eval_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = functions.build_index(storage_path, 'token_splitter', documents, [sentence_splitter, title_extractor, qa_extractor])\n",
    "base_engine = base_index.as_query_engine(similarity_top_k=5, node_postprocessors=[bge_rereanker_base])\n",
    "result_base, eval_base = functions.evaluation('querying-base-multi_context', base_engine, testset[testset.evolution_type == 'multi_context'], eval_model)\n",
    "eval_base, eval_base_scores = functions.add_evaluation(client, 'querying-base-multi_context', eval_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_index = functions.build_index(storage_path, 'token_splitter', documents, [sentence_splitter, title_extractor, qa_extractor])\n",
    "large_engine = large_index.as_query_engine(similarity_top_k=5, node_postprocessors=[bge_reranker_large])\n",
    "result_large, eval_large = functions.evaluation('querying-large', large_engine, testset, eval_model)\n",
    "eval_large, eval_large_scores = functions.add_evaluation(client, 'querying-large', eval_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_index = functions.build_index(storage_path, 'token_splitter', documents, [sentence_splitter, title_extractor, qa_extractor])\n",
    "large_engine = large_index.as_query_engine(similarity_top_k=5, node_postprocessors=[bge_reranker_large])\n",
    "result_large, eval_large = functions.evaluation('querying-large-simple', large_engine, testset[testset.evolution_type == 'simple'], eval_model)\n",
    "eval_large, eval_large_scores = functions.add_evaluation(client, 'querying-large-simple', eval_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_index = functions.build_index(storage_path, 'token_splitter', documents, [sentence_splitter, title_extractor, qa_extractor])\n",
    "large_engine = large_index.as_query_engine(similarity_top_k=5, node_postprocessors=[bge_reranker_large])\n",
    "result_large, eval_large = functions.evaluation('querying-large-reasoning', large_engine, testset[testset.evolution_type == 'reasoning'], eval_model)\n",
    "eval_large, eval_large_scores = functions.add_evaluation(client, 'querying-large-reasoning', eval_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_index = functions.build_index(storage_path, 'token_splitter', documents, [sentence_splitter, title_extractor, qa_extractor])\n",
    "large_engine = large_index.as_query_engine(similarity_top_k=5, node_postprocessors=[bge_reranker_large])\n",
    "result_large, eval_large = functions.evaluation('querying-large-multi_context', large_engine, testset[testset.evolution_type == 'multi_context'], eval_model)\n",
    "eval_large, eval_large_scores = functions.add_evaluation(client, 'querying-large-multi_context', eval_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types.append('Raranker base')\n",
    "scores.append(eval_base)\n",
    "\n",
    "types.append('Raranker large')\n",
    "scores.append(eval_large)\n",
    "\n",
    "evals = functions.create_results(types, scores)\n",
    "functions.plot_aggregate_evaluation(evals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pjatk-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
